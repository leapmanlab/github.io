<!doctype html>
<html lang="">
	<head>
		<title>Content-Aware Computation for Optical Microscopy</title>
		<meta charset="utf-8">
        <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">

        
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/reveal.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/theme/white.min.css">
        <link type="text/css" rel="stylesheet" href="index.css">


    </head>

    <body>

    	<div class="reveal">
    		<div class="slides">


    			<section data-background="linear-gradient(#32629780, #65666a0a)">
    				<h3>Content-Aware Computation for Optical Microscopy</h3>
                    <hr/>
                    <br/>
                    NIH.AI Workshop: Optical Microscopy
                    <br/><br/>
    				Matthew Guay, NIH/NIBIB
    			</section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Content-aware computation

                        - Currently, convolutional neural networks (CNNs) for image restoration and computer vision.

                        - **Image restoration**: Image denoising, spatial and spectral deconvolution, superresolution, registration, and more!

                        - ** Computer vision**: Object detection and segmentation.
                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### The big picture

                        - **Current applications**: What problems are solved today?

                        - **Computational tools**: What algorithms drive solutions?

                        - **Trust**: Can we rely on data-driven methods?
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <h2>Current applications</h2>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Image denoising

                        - Photon budgets trade off between depth, intensity, and spatial and temporal resolution.

                        - **Main idea**: Cheat the trade-off by restoring image features from low intensity acquisitions.

                        - Seminal paper: Content-Aware Image Restoration: Pushing the Limits of Fluorescence Microscopy (Weigert et al., 2018) [[PDF](https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf)]

                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/care_flatworm.png" alt="CARE network restoring a low-SNR flatworm image">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. Restoring flatworm stained nuclei from low SNR images.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/zoom_flatworm1.png" alt="Comparison of flatworm noisy data, CARE restoration, and ground truth">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. Medium-zoom comparison between a low-SNR flatworm image, CARE restoration, and high-SNR ground truth.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/zoom_flatworm2.png" alt="Comparison of flatworm noisy data, CARE restoration, and ground truth">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. High-zoom comparison between a low-SNR flatworm image, CARE restoration, and high-SNR ground truth.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/care_beetle.png" alt="CARE network restoring a low-SNR beetle embryo image">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. Restoring red flour beetle embryo stained nuclei from low SNR images.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Image superresolution

                        - **Main idea**: Improve image resolution beyond raw acquisition limits.

                        - Resolution artifacts may be due to diffraction limits, hardware limits, sparse axial sampling, others?

                        - Learn statistical superresolution models from synthetic data and alternate superresolution techniques (STORM, PALM).
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/superres1.png" alt="Diffraction-limited structure enhancement: rat INS-1 cell 1">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. CARE enhancement of diffraction-limited secretory granules and microtubules in rat INS-1 cells.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/superres2.png" alt="Diffraction-limited structure enhancement: rat INS-1 cell 2">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. CARE enhancement of diffraction-limited secretory granules and microtubules in rat INS-1 cells.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/superres4.png" alt="Superresolution to correct axial anisotropy in a zebrafish retina image">

                    <br/>

                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. CARE superresolution to correct axial anisotropy in a zebrafish retina image.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/superres3.png" alt="Additional superresolution examples">

                    <br/>
                    <small>Additional superresolution examples from the <a href="https://pdfs.semanticscholar.org/0398/b175a01c68b08272f2cf998837a1650eb946.pdf">(Belthangady et al., 2019)</a> review paper.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Computer vision

                        - Image restoration: Learn a statistical model to infer a high-quality image from low quality.

                        - **Computer vision**: Parse the structural content of an image into constituent objects.

                        - Classic techniques (thresholding, watershedding) work in many cases.

                        - CNNs may improve results in challenging cases, allow novel applications.
                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Image segmentation - instance

                        - **Instance segmentation**: Each distinct object gets a unique label.

                        <img class="stretch" src="img/ex_instance.png" alt="Instance segmentation demo">

                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Image segmentation - semantic

                        - **Semantic segmentation**: Each "type" of object gets a label, label each pixel.

                        <img class="stretch" src="img/ex_semantic.png" alt="Semantic segmentation demo">

                        <small>Examples adapted from (von Chamier et al., 2019) [[PDF](https://portlandpress.com/biochemsoctrans/article-pdf/47/4/1029/850822/bst-2018-0391c.pdf)] </small>
                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Application: blood cell segmentation

                        - One CNN to segment red blood cells (RBC), another to diagnose malaria presence from segmented cells.

                        <img class="strootch" src="img/malaria.png" alt="Segmentation of red blood cells for malaria detection">

                        <small>(Rajaraman et al., 2018) [[PDF](https://peerj.com/articles/4568.pdf)]. RBC slide, its segmentation, and an overlay. </small>
                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Application: label-free fluorescence imaging

                        - Train a CNN to predict fluorescence targets from bright-field images

                        - Advantage: Predict multiple fluorescence labels at once when that is impractical physically.

                        - Basically, semantic segmentation with fluorescence-derived class labels.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/labelfree1.png" alt="Label-free fluorescence imaging: training">

                    <br/>
                    <small>(Ounkomol et al., 2018)[<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6212323/pdf/nihms-1501928.pdf">PDF</a>]. Label-free prediction model training on a fluorescence target.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/labelfree2.png" alt="Label-free fluorescence imaging: prediction">

                    <br/>
                    <small><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6212323/pdf/nihms-1501928.pdf">(Ounkomol et al., 2018)</a>. Multi-target prediction across a time series by combining individually-trained networks.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <h2>Computational tools</h2>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Convolutional neural networks

                        - **Main idea**: Learn convolutional features from image data to solve a classification task.

                        - Sequential convolution operations + nonlinearities (ReLU) + pooling: learn abstract image-scale features to solve, e.g., classification problems.

                        - Idea existed since the 90's, resurgence since the 2012 Alexnet results for natural image classification (Krizhevsky et al., 2012) [[PDF](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)]
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/cnn1.png" alt="Example of a CNN architecture">

                    <br/>
                    <small><a href="https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf">Source</a>. Example of a small CNN architecture. Convolutional features are learned and transformed into classification predictions about image content.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Convolution for images

                        - Idea: Slide a little window along an image, do some computation in each window.

                        - The computation: dot product between a window-sized **kernel** and the image.

                        - Example: Gaussian blurring convolves a Gaussian kernel with an image.

                        - CNNs learn kernels. Learned kernels are features, convolution output is a **feature map**.

                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/conv_animation.gif" alt="Example of a 2D convolution operation">

                    <br/>
                    <small><a href="https://commons.wikimedia.org/wiki/File:3D_Convolution_Animation.gif">Source</a>. 2D convolution example. A sharpening kernel is applied to an "image".</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Activation functions

                        - Linear transforms like convolutions are followed by a nonlinear **activation function**.

                        - Examples: **ReLU** is `$\max(0, x)$`, sigmoid is `$1/\left(1+e^{-x}\right)$`.

                        - Networks with nonlinearities are more expressive: a sequence of linear transforms is just another linear transform.
                    </textarea>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Spatial pooling

                        - Downsample an image or feature map to produce a "zoomed-out" view.

                        - Convolution ops applied to downsampled data find patterns across larger spatial scales.

                        - **Max pooling**: replace 2x2 pixel block with the single largest value.

                        - Mean pooling: replace 2x2 pixel block with the average block value.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/conv_viz.jpg" alt="Visualizing layers in a simple CNN">

                    <br/>
                    <small><a href="http://cs231n.github.io/convolutional-networks/">Source</a>. Visualization of layers inside a small image classification CNN.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### U-Nets

                        - Vanilla CNNs provide spatially-coarse classification.

                        - What about spatially-dense image problems? (Segmentation, restoration, etc.)

                        - **U-Net**: Use convolutional encoder + decoder with skip connections for _image-to-image translation_.

                        - Seminal paper: (Ronneberger et al., 2015) [[PDF](https://arxiv.org/pdf/1505.04597.pdf)]
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/unet.png" alt="Example of a U-Net architecture">

                    <br/>
                    <small><a href="https://arxiv.org/pdf/1505.04597.pdf">(Ronneberger et al., 2015)</a>. Diagram of the original U-Net architecture.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/unet2.png" alt="Segmentation with U-Net">

                    <br/>
                    <small><a href="https://arxiv.org/pdf/1505.04597.pdf">(Ronneberger et al., 2015)</a>. Image segmentation example with the U-Net. Blue input tiles provide spatial context for yellow output tiles. Large images can be segmented tile by tile.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Generative Adversarial Networks

                        - Simultaneously train two networks, a generator (e.g. U-Net) and discriminator.

                        - Generator trained to perform a task (e.g. image-to-image translation).

                        - Discriminator trained to distinguish generator output from real training samples.

                        - When successful, GAN generators produce more realistic output than without adversarial training.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/gancomparison.png" alt="Comparison of image-to-image translation networks">

                    <br/>
                    <small><a href="https://pdfs.semanticscholar.org/0398/b175a01c68b08272f2cf998837a1650eb946.pdf">(Belthangady et al., 2019)</a>. Comparison of image-to-image translation networks. A U-Net builds on an encoder-decoder by adding skip connections between convolution blocks. A GAN builds on a generator by forcing its output to be indistinguishable from training samples.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <h3>Example: <a href="https://www.thispersondoesnotexist.com">thispersondoesnotexist.com</a></h3>

                    <iframe class="stretch" data-src="https://thispersondoesnotexist.com"></iframe>

                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/superres3.png" alt="Additional superresolution examples">

                    <br/>
                    <small><a href="https://pdfs.semanticscholar.org/0398/b175a01c68b08272f2cf998837a1650eb946.pdf">(Belthangady et al., 2019)</a>. Two superresolution approaches use GANs to force (fast) trainable network outputs to be indistinguishable from (slow) ground truth high-resolution computations.</small>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <h2>Trust</h2>

<!--                    CARE paper discusses parametric voxel distribution prediction and quantifying ensemble disagreement.-->

<!--                    (Xue et al., 2019) uses a Bayesian CNN to quantify CNN prediction uncertainty.-->

<!--                    Anything else?-->
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Can content-aware methods be trusted?

                        - **Bottom line**: **No**. Not always, not right now.

                        - Neural network image restoration may fail subtly on new data or anomalous structures.

                        - Long term, work on network interpretability will probably make results more trustworthy.

                        - For now, validate like you would validate a human, not an algorithm.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/trust1.png" alt="Network trust example: word restoration">

                    <br/>
                    <small><a href="https://pdfs.semanticscholar.org/0398/b175a01c68b08272f2cf998837a1650eb946.pdf">(Belthangady et al., 2019)</a>. Demonstration of the pitfalls of data-driven image restoration. An image of the word "Witenagemot" is restored by U-Nets trained on different datasets. Each network produces a visually-plausible answer, but only the one trained on the right dataset produces a correct answer.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Model interpretability

                        - Approach 1: **model interpretability**.

                        - How does a network reach its prediction? What image features most contribute to it?

                        - Seminal paper: (Olah et al., 2018) [[link](https://distill.pub/2018/building-blocks/)] for natural image classification.

                        - No rock-solid solutions yet, research is ongoing.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">

                    <iframe class="stretch" data-src="https://distill.pub/2018/building-blocks/"></iframe>

                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Confidence prediction

                        - Approach 2: **predicting network confidence**.

                        - Instead of predicting a single pixel value, predict a per-pixel parametric statistical model.

                        - [(Weigert et al., 2018)](https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf) tries predicting mean μ and std σ of Laplace distributions per pixel.

                        - Goal: Low σ = high prediction confidence?
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/trust2.png" alt="Quantifying network confidence by predicting probability distributions">

                    <br/>
                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. Predicting per-pixel probability distributions allows one to build pseudo-confidence intervals for network predictions. Here, even when ground-truth values differ from predictions, they fall within the predicted confidence interval. Still no guarantees the learned probability distributions converge to anything physical.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Ensemble agreement

                        - Approach 3: **measuring ensemble disagreement**.

                        - Ensemble: Collection of statistical models (neural nets or otherwise).

                        - Train multiple instances of the same model from initial conditions.

                        - Measure how well the models agree on a prediction.

                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/trust3.png" alt="Using ensemble disagreement to measure prediction confidence">

                    <br/>
                    <small><a href="https://www.biorxiv.org/content/biorxiv/early/2018/07/03/236463.full.pdf">(Weigert et al., 2018)</a>. Using ensemble disagreement to measure prediction confidence.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Bayesian neural networks

                        - More sophisticated approach: **Bayesian CNNs**.

                        - Learn probability distributions for weights to quantify _model uncertainty_.

                        - Predict probability distributions for restored pixels to quantify _data uncertainty_.

                        - See ([Xue et al., 2019](https://www.osapublishing.org/DirectPDFAccess/DF2983E0-90AF-0DDB-EBACECC4047414E8_412113/optica-6-5-618.pdf)) for an application to phase imaging microscopy.
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <img class="stretch" src="img/trust4.png" alt="Quantifying network confidence with bayesian neural network ensembles">

                    <br/>
                    <small><a href="https://www.osapublishing.org/DirectPDFAccess/DF2983E0-90AF-0DDB-EBACECC4047414E8_412113/optica-6-5-618.pdf">(Xue et al., 2019)</a>. The authors use ensembles of Bayesian CNNs to quantify uncertainty relative to the data and relative to the model.</small>
                </section>


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Conclusion

                        - Content-aware computation lets microscopists cheat physical limits of sample acquisition.

                        - This enables scientific experiments that are impossible otherwise.

                        - Results cannot be entirely trusted, may fail in subtle ways, trustworthiness will probably improve.

                        - **Question**: What do you do today when the problem you want to solve requires content-aware tools?
                    </textarea>
                </section>


                <section data-background="linear-gradient(#32629780, #65666a0a)">
                    <h1>Thank you!</h1>
                </section>



    		</div>
    	</div>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/js/reveal.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/marked.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/markdown.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/math/math.js"></script>

        
        <script type="text/javascript">
            Reveal.initialize({

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js',
                    config: 'TeX-AMS_HTML-full'
                },

                // Factor of the display size that should remain empty around the content
                margin: 0.03,

                // Display controls in the bottom right corner
                controls: true,

                // Display a presentation progress bar
                progress: true,

                // Set default timing of 2 minutes per slide
                defaultTiming: 90,

                // Display the page number of the current slide
                slideNumber: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,

                // Change the presentation direction to be RTL
                rtl: false,

                // Randomizes the order of slides each time the presentation loads
                shuffle: false,

                // Turns fragments on and off globally
                fragments: true,

                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Flags if we should show a help overlay when the questionmark
                // key is pressed
                help: true,

                // Flags if speaker notes should be visible to all viewers
                showNotes: false,

                // Global override for autolaying embedded media (video/audio/iframe)
                // - null: Media will only autoplay if data-autoplay is present
                // - true: All media will autoplay, regardless of individual setting
                // - false: No media will autoplay, regardless of individual setting
                autoPlayMedia: null,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Use this method for navigation when auto-sliding
                autoSlideMethod: Reveal.navigateNext,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition style
                transition: 'slide', // none/fade/slide/convex/concave/zoom

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

                // Number of pixels to move the parallax background per slide
                // - Calculated automatically unless specified
                // - Set to 0 to disable movement along an axis
                parallaxBackgroundHorizontal: null,
                parallaxBackgroundVertical: null,

                // The display mode that will be used to show slides
                display: 'block'
            });
        </script>

    </body>
        
</html>