<!doctype html>
<html lang="">
	<head>
		<title>Putting Deep Learning to Work for EM</title>
		<meta charset="utf-8">
        <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">

        
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/reveal.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/theme/white.min.css">
        <link type="text/css" rel="stylesheet" href="index.css">


    </head>

    <body>

    	<div class="reveal">
    		<div class="slides">


<!-- 
                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                    </textarea>
                </section>
 -->


    			<section data-background="linear-gradient(#32629780, #65666a0a)">
    				<h2>Putting Deep Learning to Work for EM</h3>
                    <hr/><br/>
                    November 13, 2020<br/><br/>
    				Matthew Guay<br/>
                    <a href="mailto:matthew.guay@nih.gov" target="_blank">matthew.guay@nih.gov</a>

				    <br/><br/>
				    <a href="https://leapmanlab.github.io/fars1120/" target="_blank">https://leapmanlab.github.io/fars1120/</a>
    			</section>
		    


                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        - Electron microscopy (EM) computer vision challenges center around **segmentation**.

                        - Convolutional neural networks (CNNs) are starting to make algorithmic segmentation viable in theory. 

                        - Practice should translate to building larger, more detailed 3D structural models much faster. 

                        - **Main question**: How to accelerate EM image segmentation in practice?

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        - Answer via case study: what's going on in our lab?

                        - Laboratory of Cellular Imaging and Macromolecular Biophysics (**LCIMB**).

                        - Dr. Matthew Guay working on algorithmic segmentation R&D for EM.
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Overview

                        - What LCIMB has done.

                        - What LCIMB is doing.

                        - What LCIMB wants to do.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ## What LCIMB has done

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Dense cellular segmentation

                        - "Holy grail" goal: 3D structural models of all cells and all resolvable subcellular structures in a large volume.

                        - **Current achievement**: Good but imperfect multi-class semantic segmentation in a human platelet sample.

                        - Preprints: <a href="https://heyitsguay.github.io/doc/2d3d-main.pdf" target="_blank">[Main Paper]</a>.  <a href="https://heyitsguay.github.io/doc/2d3d-supplementary.pdf" target="_blank">[Supplementary]</a>.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)" data-transition="slide-in none-out">
                    <textarea data-template>

                        <img style="max-width: 80%; max-height: 80%" src="img/densecell-before.png" alt="SBF-SEM image of human platelet cells">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)" data-transition="none-in slide-out">
                    <textarea data-template>

                        <img style="max-width: 80%; max-height: 80%" src="img/densecell-after.png" alt="Segmented SBF-SEM image of human platelet cells">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <video width="500" height="500" controls autoplay muted loop><source data-src="img/cellanim.mp4" type="video/mp4" /></video>
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Technical challenges

                        - **Anisotropy**: 10nm `$y-x$`, 40nm `$z$` resolution.
                        - **Large images**: Tiling smaller segmentations across larger volumes.
                        - **3D Processing**: 2D vs 3D tradeoffs in memory, processing, spatial context.
                        - **Training data**: Tedious to create training labels.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Data setup

                        - Taken from two human blood donors, imaged on Gatan 3View SBF-SEM.

                        - **Training** data: `$50\times 800\times 800$` subvolume from Donor 1. 

                        - **Eval** data: `$24\times 800\times 800$` subvolume from Donor 1 (different cells)

                        - **Test** data: `$121\times 609\times 400$` and `$110\times 602 \times 509$` subvolumes from Donor 2.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/data-training.png" alt="Training data sample">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/data-eval.png" alt="Eval data sample">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/data-test.png" alt="Test data samples">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Software stack

                         - TensorFlow + Python.

                         - Local workstation: Ubuntu with an NVIDIA GTX 1080.

                         - Training and inference on Biowulf as well.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Data augmentation

                        - **Elastic deformation**: Wiggle biological blobs to create somewhat different blobs.
                        - **Isometries**: Rotate and reflect images.
                        - **Intensity augmentation**: Tweak brightness and contrast `$\pm 10\%$`.
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Loss weighting

                        - Training loss is a weighted sum of per-voxel cross entropy.

                        - **Class balancing**: Upweight regions belonging to rare classes.

                        - **Edge preserving**: Upweight regions where multiple cells or organelles are in close proximity.

                        - Generated edge preserving weights directly from label data with no morphological information, using thresholded diffusion operations.  
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Training error weights

                        <img style="max-width: 100%; max-height: 100%" src="img/weight-ex.png" alt="Example weight image">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Payoff: better cell separation

                        <img style="max-width: 100%; max-height: 100%" src="img/cdeep3m-comparison.png" alt="Comparison between CDeep3M and our results">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Full training loss

                        <img style="max-width: 100%; max-height: 100%" src="img/loss.png" alt="Complete training loss function">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Architecture selection

                        - **First step**: Literature review. We tried U-Nets (2D and 3D), DeeplabV3.

                        - Doing better: Stick together better modules, **validate** whether your choices help.

                        - Final result: **2D-3D+3x3x3** net.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### 2D-3D+3x3x3 net

                        <img style="max-width: 100%; max-height: 100%" src="img/2d3d-net.png" alt="2D-3D+3x3x3 net architecture">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Validation challenges

                        - **Eval metric randomness** induced by random weight initialization.

                        - Likely due to small training datasets.

                        - Brute force solution: Train multiple net instances with different initializations.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Architecture comparison

                        <img style="max-width: 100%; max-height: 100%" src="img/arch-validation.png" alt="Eval MIoU histograms for net variants">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Ensembling

                        - Architecture validation generates several high-performing instances per architecture.

                        - **Instance ensembling**: run multiple good instances, average predictions.

                        - Simple idea, big performance boost.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 80%; max-height: 80%" src="img/ensembling.png" alt="Eval MIoU histograms for net variants">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Final results

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/final-results.png" alt="Final results on test data">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### What's missing

                        - **Better accuracy**: Enough said.

                        - **Better generalization**: We see a significant performance drop on Donor 2.

                        - **Instance segmentation**: Identify objects, not just voxel classes.

                        - **Workflow integration**: Bring training, inference, and manual correction into one tool for fast feedback.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Label dataset bootstrapping

                        - **Goal**: Segment large image volumes to gold-standard accuracy with as little manual labor as possible.

                        <!-- - Shift in objective: Minimize time to gold-standard accuracy (by any means necessary) instead of maximizing unassisted accuracy. -->

                        - **Bootstrapping**: 
                            1. "Seed" a label dataset with a small manually-labeled region. 
                            2. Iteratively train networks on labels, segment surrounding regions, and correct.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        ### Proof of concept result

                        - ISBI 2021 submission on platelet cell membrane segmentation. <a href="https://heyitsguay.github.io/doc/isbi2021-draft.pdf" target="_blank">[Draft]</a>

                        - Headline: **131x average decrease** in image labeling time between initial seed region and final algorithm assistance. 

                        - Algorithmic refinements, training dataset expansion over 4 **bootstrap iterations**.
                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)" data-transition="slide-in none-out">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/cell-membrane-before.png" alt="SBF-SEM image of human platelet cells">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)" data-transition="none-in slide-out">
                    <textarea data-template>

                        <img style="max-width: 100%; max-height: 100%" src="img/cell-membrane-after.png" alt="Human platelet cells with cell membranes labeled">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 66%; max-height: 66%" src="img/bi-results.png" alt="Labeling acceleration results">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Software stack

                        - PyTorch + Python. More friendly than Tensorflow, better code.

                        - Manual labeling and label correction done using GNU Image Manipulation Program.

                        - Same computer setup as before.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Image preprocessing

                        - Unsharp filtering then median blurring seems to help learning, especially when training dataset size is small.

                        <img style="max-width: 66%; max-height: 66%" src="img/filtering.png" alt="Preprocessing filtering example">


                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Prediction error weighting

                        - BI `$n$`'s training data is the corrected output of previous BIs, so we know where nets make mistakes.

                        - **Prediction error weighting**: Upweight regions where previous BI's nets made mistakes.

                        - Ended up upweighting only false negatives, the more costly error to correct.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 60%; max-height: 60%" src="img/pred-error.png" alt="Prediction error weighting example">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### More ensembling

                        - In addition to instance ensembling, used **tile overlap ensembling**.

                        - For tiled image volumes, predict highly-overlapping tiles and average predictions per-voxel.

                        - Tradeoff: increased runtime, though could be parallelized.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Thin 3D U-Net

                        - For simplicity's sake, went with a **Thin 3D U-Net**.

                        - Alter a standard 2D U-Net to use 3D conv ops padded in `$z$`, and no `$z$` spatial pooling or upsampling.

                        - Fix a small $z$ value for all net tensors, including input and output (e.g. $z=5$).

                        - More advantageous for anisotropic SBF-SEM, but also provides 3D context with lower memory footprint.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Testing spatially-dependent performance

                        - **Main claim**: iterative label volume expansion beats random labeling because overfit nets perform better on new data spatially near training data.

                        - We tested by validating each BI's algorithm performance on two eval regions: _eval-near_ near the training region, _eval-far_ far from the training region.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>

                        <img style="max-width: 50%; max-height: 50%" src="img/bootstrap-graph.png" alt="Eval near vs far performance comparison">

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### What's missing

                        - **Faster retraining**: Majority of errors corrected in each BI were correlated from slice to slice. Updating nets after each slice for faster correction.

                        - **Retrain optimization**: Much to be done to decrease retraining time after each new batch of corrections.

                        - **Workflow integration**: Clunky workflow of manually moving files between the file system and the image processing program can be refined.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### What LCIMB is doing now

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Dataset expansion

                        - Currently amassing a number of platelet FIB-SEM and SBF-SEM datasets.

                        - Differences in resolution, image quality, platelet biology, extracellular material.

                        - How to segment robustly across these differences?

                        - How to deal with "long-tail" structures found in larger image volumes?

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Panoptic segmentation

                        - **Panoptic segmentation**: Combination of semantic and instance segmentation, usually in a single computation graph.

                        - Experiments with 2D and 3D Mask R-CNN for platelet cells and organelles are OK in 2D, bad in 3D.

                        - RPN modules are a pain. **Foolish hope**: Replace RPN modules with biophysical boundary detection for cellular panoptic segmentation.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### COVID-10 thrombus modeling

                        - **Goal**: Build structural models of platelet cells in human blood samples from COVID-19 and control patients.

                        - Technical challenge: Scaling nanoscale-resolution segmentation to 100s of cells per physical sample.

                        - Population comparison inference challenges whether sample sizes are large or small.

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Seeking new EM collaborations

                        - LCIMB has a Gatan 3View **SBF-SEM** with low utilization.

                        - **FIB-SEM** incoming soon, too.

                        - We are interested in new collaborations that leverage large volume imaging capabilities.

                        - I am interested in samples combining multiple cell types, and systems outside the large-volume EM spotlight. 

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        ### Expanding IRP microscopy AI

                        - I am interested in helping labs hire computational staff, and connecting existing computational staff to computer vision training resources.

                        - **Considering a project?** Let's talk about your data and your goals during discussion!

                    </textarea>
                </section>



                <section data-markdown data-background="linear-gradient(#32629780, #65666a0a)">
                    <textarea data-template>
                        # THANK YOU!

                    </textarea>
                </section>

                

    		</div>
    	</div>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/js/reveal.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/marked.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/markdown/markdown.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/plugin/math/math.js"></script>

        
        <script type="text/javascript">
            Reveal.initialize({

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js',
                    config: 'TeX-AMS_HTML-full'
                },

                // Factor of the display size that should remain empty around the content
                margin: 0.03,

                // Display controls in the bottom right corner
                controls: true,

                // Display a presentation progress bar
                progress: true,

                // Set default timing of 2 minutes per slide
                defaultTiming: 90,

                // Display the page number of the current slide
                slideNumber: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,

                // Change the presentation direction to be RTL
                rtl: false,

                // Randomizes the order of slides each time the presentation loads
                shuffle: false,

                // Turns fragments on and off globally
                fragments: true,

                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Flags if we should show a help overlay when the questionmark
                // key is pressed
                help: true,

                // Flags if speaker notes should be visible to all viewers
                showNotes: false,

                // Global override for autolaying embedded media (video/audio/iframe)
                // - null: Media will only autoplay if data-autoplay is present
                // - true: All media will autoplay, regardless of individual setting
                // - false: No media will autoplay, regardless of individual setting
                autoPlayMedia: null,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Use this method for navigation when auto-sliding
                autoSlideMethod: Reveal.navigateNext,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition style
                transition: 'slide', // none/fade/slide/convex/concave/zoom

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

                // Number of pixels to move the parallax background per slide
                // - Calculated automatically unless specified
                // - Set to 0 to disable movement along an axis
                parallaxBackgroundHorizontal: null,
                parallaxBackgroundVertical: null,

                // The display mode that will be used to show slides
                display: 'block'
            });
        </script>

    </body>
        
</html>
